#version 430 core

#extension GL_KHR_shader_subgroup_basic  : enable
#extension GL_KHR_shader_subgroup_ballot : enable
#extension GL_KHR_shader_subgroup_arithmetic : enable

struct PGTO {
    float coeff;
    float alpha;
    uint ijkl;
};

struct CGTO {
    vec3 coord;
    uint pgto_off;
    uint pgto_len;
};

layout (binding = 0, std430) readonly restrict buffer CGTO_XYZR_BUFFER {
    vec4 cgto_xyzr[];
};
layout (binding = 1, std430) readonly restrict buffer CGTO_OFFSET_BUFFER {
    uint cgto_offset[];
};
layout (binding = 2, std430) readonly restrict buffer PGTO_COEFF_BUFFER {
    float pgto_coeff[];
};
layout (binding = 3, std430) readonly restrict buffer PGTO_ALPHA_BUFFER {
    float pgto_alpha[];
};
layout (binding = 4, std430) readonly restrict buffer PGTO_RADIUS_BUFFER {
    float pgto_radius[];
};
layout (binding = 5, std430) readonly restrict buffer PGTO_IJKL_BUFFER {
    uint pgto_ijkl[];
};

layout (binding = 6, std430) readonly restrict buffer D_BUFFER {
    float D_matrix[];
};

layout(std140, binding = 0) uniform UniformBlock {
    mat4 world_to_model;
    mat4 index_to_world;
    vec4 step;
    uint D_matrix_dim;
};

layout(binding = 0) writeonly restrict uniform image3D out_vol;

#if 0
float safe_pow(float base, uint exponent) {
    switch(exponent) {
        case 0u: return 1.0;
        case 1u: return base;
        case 2u: return base * base;
        case 3u: return base * base * base;
        case 4u: {
            float b2 = base * base;
            return b2 * b2;
        }
        default: {
            return 1.0;
        }
    }
}
#else
float safe_pow(float base, uint exponent) {
    // Handles any non-negative integer exponent
    float v = 1.0;
    for (uint e = 0u; e < exponent; ++e) {
        v *= base;
    }
    return v;
}
#endif

uvec4 unpack_ijkl(in uint ijkl) {
    return uvec4(
        (ijkl >>  0) & 0xFFu,
        (ijkl >>  8) & 0xFFu,
        (ijkl >> 16) & 0xFFu,
        (ijkl >> 24) & 0xFFu
    );
}

uint unpack_offset(uint x) {
    return x & 0x00FFFFFFu;
}

uint unpack_length(uint x) {
    return x >> 24u;
}

uint pack_offset_length(uint offset, uint length) {
    return (length << 24u) | offset;
}

#define WG_SIZE 512
#define TILE_SIZE 32
#define TILE_AREA (TILE_SIZE*TILE_SIZE)
#define MAX_SCREENED_CGTOS 2048
#define MAX_TILE_PGTOS (TILE_SIZE * 16)
#define INVALID_CGTO_IDX 0xFFFFFFFFu

shared uint  screened_cgtos[MAX_SCREENED_CGTOS];
shared uint  num_screened_cgtos;

shared float D_tile[TILE_SIZE][TILE_SIZE];
shared CGTO  cgtos_tile[TILE_SIZE];
shared PGTO  pgtos_tile[MAX_TILE_PGTOS];
shared uint  num_pgtos;

// Calculates the D matrix index for the upper triangular symmetric matrix storage
uint get_index(uint i, uint j, uint N) {
    uint row = (i < j) ? i : j;   // smaller index
    uint col = (i < j) ? j : i;   // larger index
    uint row_offset = row * (2 * N - row + 1) / 2;
    return row_offset + (col - row);
}

// Populate the D GSM from the global D_matrix
void fill_D_tile(uint tile_i, uint tile_j) {
    uint tid = gl_LocalInvocationIndex;
    if (tid >= TILE_SIZE) return;

    uint baseI = tile_i * TILE_SIZE;
    uint baseJ = tile_j * TILE_SIZE;

    uint idxI = baseI + tid;
    uint gi = (idxI < MAX_SCREENED_CGTOS) ? screened_cgtos[idxI] : INVALID_CGTO_IDX;

    for (uint col = 0; col < TILE_SIZE; ++col) {
        uint idxJ = baseJ + col;
        uint gj = (idxJ < MAX_SCREENED_CGTOS) ? screened_cgtos[idxJ] : INVALID_CGTO_IDX;

        float value = 0.0;
        if (gi != INVALID_CGTO_IDX && gj != INVALID_CGTO_IDX) {
            value = D_matrix[get_index(gi, gj, D_matrix_dim)];
        }
        D_tile[tid][col] = value;
    }
}

// Populate CGTO and PGTO GSM with CGTOs found in tile
void fill_cgtos_tile(uint tile_number, vec3 model_aabb_min, vec3 model_aabb_max) {
    const uint tid  = gl_LocalInvocationIndex;

    // Only first TILE_SIZE threads participate
    if (tid >= TILE_SIZE) return;

    // Global CGTO index for this tile position
    uint global_cgto_idx = screened_cgtos[tile_number * TILE_SIZE + tid];

    vec3 cgto_center = vec3(0.0);
    uint cgto_pgto_off = 0u;
    uint cgto_pgto_len = 0u;

    if (global_cgto_idx != INVALID_CGTO_IDX) {
        // Load packed offset/length
        uint src_beg = cgto_offset[global_cgto_idx];
        uint src_end = cgto_offset[global_cgto_idx + 1u];
        uint src_len = src_end - src_beg;

        if (src_beg != src_end) {
            // Reserve contiguous space for this CGTO's PGTOs
            uint dst = atomicAdd(num_pgtos, src_len);

            cgto_center = cgto_xyzr[global_cgto_idx].xyz;
            cgto_pgto_off = dst;
            
            vec3 model_xyz = vec3(world_to_model * vec4(cgto_center, 1.0));
            vec3   d = clamp(model_xyz, model_aabb_min, model_aabb_max) - model_xyz;
            float d2 = dot(d, d);

            // Copy PGTOs
            for (uint k = 0; k < src_len; ++k) {
                float r = pgto_radius[src_beg + k];
                // Cull based on radius
                if (d2 < r * r) {
                    PGTO g;
                    g.coeff = pgto_coeff[src_beg + k];
                    g.alpha = pgto_alpha[src_beg + k];
                    g.ijkl  = pgto_ijkl[src_beg + k];
                    pgtos_tile[cgto_pgto_off + cgto_pgto_len] = g;
                    cgto_pgto_len++;
                }
            }
        }
    }

    cgtos_tile[tid].coord    = cgto_center;
    cgtos_tile[tid].pgto_off = cgto_pgto_off;
    cgtos_tile[tid].pgto_len = cgto_pgto_len;
}

// One thread per CGTO
void eval_phis(out float out_phi[TILE_SIZE], vec3 coord) {
    // Initialize to zero to handle empty CGTO slots
    for (uint i = 0; i < TILE_SIZE; ++i) out_phi[i] = 0.0;

    // Compute φ_i(coord) for all CGTOs in the tile
    for (uint i = 0; i < TILE_SIZE; ++i) {
        uint pgto_off = cgtos_tile[i].pgto_off;
        uint pgto_len = cgtos_tile[i].pgto_len;

        vec3 center = cgtos_tile[i].coord;
        vec3 d  = coord - center;
        float r2 = dot(d, d);
        
        float phi = 0.0;
        for (uint j = pgto_off; j < pgto_off + pgto_len; ++j) {
            PGTO pgto = pgtos_tile[j];
            uvec4 ijkl = unpack_ijkl(pgto.ijkl);
            float fx = safe_pow(d.x, ijkl.x);
            float fy = safe_pow(d.y, ijkl.y);
            float fz = safe_pow(d.z, ijkl.z);
            phi += pgto.coeff * fx * fy * fz * exp(-pgto.alpha * r2);
        }
        out_phi[i] = phi;
    }
}

float symmetric_contraction(float phi[TILE_SIZE], float D[TILE_SIZE][TILE_SIZE]) {
    float result = 0.0;
    for (uint i = 0; i < TILE_SIZE; ++i) {
        float ai = phi[i];
        result += D[i][i] * ai * ai;                        // Diagonal
        for (uint j = i + 1; j < TILE_SIZE; ++j) {
            result += 2.0 * D[i][j] * ai * phi[j];          // Off-diagonal
        }
    }
    return result;
}

float contraction(float phi_mu[TILE_SIZE], float phi_nu[TILE_SIZE], float D[TILE_SIZE][TILE_SIZE]) {
    float result = 0.0;
    for (uint i = 0; i < TILE_SIZE; ++i) {
        float ai = phi_mu[i];
        for (uint j = 0; j < TILE_SIZE; ++j) {
            result += 2.0 * D[i][j] * ai * phi_nu[j];
        }
    }
    return result;
}

layout (local_size_x = 8, local_size_y = 8, local_size_z = 8) in;
void main() {
    uint tid = gl_LocalInvocationIndex;
    if (tid == 0) {
        num_screened_cgtos = 0;
    }
    barrier();

    vec3 model_aabb_min = vec3( gl_WorkGroupID.xyz                 * gl_WorkGroupSize.xyz) * step.xyz;
    vec3 model_aabb_max = vec3((gl_WorkGroupID.xyz + uvec3(1,1,1)) * gl_WorkGroupSize.xyz) * step.xyz;
    // Stage 1: Screening. Prune CGTOs to identify which are relevant for region
    {
        // Stream matches directly; avoid large per-thread stacks and subgroup prefix sums
        for (uint i = tid; i < D_matrix_dim; i += WG_SIZE) {
            vec4 cgto = cgto_xyzr[i];
            if (cgto.w == 0.0) continue;

            vec3 model_xyz = vec3(world_to_model * vec4(cgto.xyz, 1.0));
            float r2       = cgto.w * cgto.w;

            vec3   d = clamp(model_xyz, model_aabb_min, model_aabb_max) - model_xyz;
            float d2 = dot(d, d);

            if (d2 < r2) {
                uint idx = atomicAdd(num_screened_cgtos, 1u);
                if (idx < MAX_SCREENED_CGTOS) {
                    screened_cgtos[idx] = i;
                }
            }
        }

        barrier();

        // Clamp to capacity
        if (tid == 0) {
            if (num_screened_cgtos > MAX_SCREENED_CGTOS) {
                num_screened_cgtos = MAX_SCREENED_CGTOS;
            }
        }
        barrier();

        // Fill tail with INVALID_CGTO_IDX
        for (uint i = tid; i < MAX_SCREENED_CGTOS; i += WG_SIZE) {
            if (i >= num_screened_cgtos) {
                screened_cgtos[i] = INVALID_CGTO_IDX;
            }
        }
    }

    float phi_tile_mu[TILE_SIZE]; // evaluated φ_μ(r) in registers
    float phi_tile_nu[TILE_SIZE]; // evaluated φ_ν(r) in registers
    vec3 coord = (index_to_world * vec4(gl_GlobalInvocationID.xyz, 1.0)).xyz;
    float rho = 0.0;

    if (num_screened_cgtos == 0u) {
        // No CGTOs contribute to this region
        if (all(lessThan(gl_GlobalInvocationID.xyz, imageSize(out_vol)))) {
            imageStore(out_vol, ivec3(gl_GlobalInvocationID.xyz), vec4(0.0));
        }
        return;
    }

    // DIV UP
    uint num_tiles = (num_screened_cgtos + TILE_SIZE - 1) / TILE_SIZE;

    for (uint tile_i = 0; tile_i < num_tiles; ++tile_i) {
        // DIAGONAL TILE

        if (tid == 0) {
            num_pgtos = 0;
        }
        barrier();

        // Populate GSM
        fill_D_tile(tile_i, tile_i);
        fill_cgtos_tile(tile_i, model_aabb_min, model_aabb_max);

        barrier();

        // Evaluate into phi mu
        eval_phis(phi_tile_mu, coord);

        // Diagonal tile Contribution
        rho += symmetric_contraction(phi_tile_mu, D_tile);

        for (uint tile_j = tile_i + 1; tile_j < num_tiles; ++tile_j) {
            barrier();

            // OFF DIAGONAL TILE
            if (tid == 0) {
                num_pgtos = 0;
            }

            barrier();

            // Populate GSM
            fill_D_tile(tile_i, tile_j);
            fill_cgtos_tile(tile_j, model_aabb_min, model_aabb_max);

            barrier();

            // Evaluate into phi nu
            eval_phis(phi_tile_nu, coord);

            // Off-diagonal tile Contribution
            rho += contraction(phi_tile_mu, phi_tile_nu, D_tile);
        }
    }
    
    // Step 3: Write result
    if (all(lessThan(gl_GlobalInvocationID.xyz, imageSize(out_vol)))) {
        imageStore(out_vol, ivec3(gl_GlobalInvocationID.xyz), vec4(rho));
    }
}